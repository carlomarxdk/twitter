{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wrIj1OxTOAxq"
   },
   "source": [
    "Links to the website: [Start](https://carlomarxdk.github.io/twitter/) || [Overview](https://carlomarxdk.github.io/twitter/general.html) || [Network](https://carlomarxdk.github.io/twitter/network.html) || [Vocabulary and Sentiments](https://carlomarxdk.github.io/twitter/sentiment.html) || [*Hashtags*](https://carlomarxdk.github.io/twitter/hashtags.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYLUj2WuaYeA"
   },
   "source": [
    "#1.  Motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1PpnGqYajI0"
   },
   "source": [
    "Recently **Tweeter** has released the [dataset that contains 3 million tweets](https://about.twitter.com/en_us/values/elections-integrity.html#data) posted by troll accounts. It is believed that accounts were used to propagate fake news, manipulate public opinion, spread rumors etc. \n",
    "\n",
    "The dataset contains huge amount of information including:\n",
    "\n",
    "* text\n",
    "* date and time markers\n",
    "* location markers\n",
    "* mentions, retweets, likes, comments count\n",
    "* mames of the mentioned or retweeted accounts \n",
    "* etc\n",
    "\n",
    "Thus, considering the amount of  data, this set was considered to be a good base for the network analysis (e.g. connections, distributions, communities and so on ). More than that, this set allows to apply some of the **natural language processing** tecnhiques to analyze the content of tweets (e.g. word frequencies, hashtags, sentiments).\n",
    "\n",
    "Our **goal** was to present the results of the analysis in user-friendly way by using interactive graphs and plots. I.e. we wanted to show ...... WE NEED TO ADD SOMETHING HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n57OtfgrajBb"
   },
   "source": [
    "# 2. Basic Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WHfnSz-YajK6"
   },
   "source": [
    "The dataset was mostly cleaned and preprocessed by Twitter (i.e. it was ready to be used for the analysis). However, due to the large size fo the dataset, we had to perfrom several procedures to lower the size of data for different stages of the analysis: \n",
    "\n",
    " For the **Hashtags Analysis** part:\n",
    "*  We extracted and separatelly stored the hashtags use per day (for the period November 2009 untill November 2018),\n",
    "*  We separatelly calculated and stored hasthags usage per months\n",
    "*  We separatelly stored  TF-IDF statistics for all tweets and for English Tweets\n",
    "\n",
    "For the **Vocabulary Analysis** part:\n",
    "*  We extracted language, content (text), and time of the tweet,\n",
    "*  We excluded all tweets in other language than English,\n",
    "*  We cleaned each tweet from twitter handles (retweets and mentions of other users), URLs, hashtags, and emojis\n",
    "\n",
    "## General Dataset Statistics\n",
    "\n",
    "**Data**:\n",
    "* 5.3 GB of size\n",
    "* 10 million entries (entry per tweet)\n",
    "* 31 variables (including location, language, likes, retweets, hashtags etc.)\n",
    "* Covers period from November 2009 till November 2018\n",
    "* Tweets were written in 11 different languages\n",
    "\n",
    "**Network:**\n",
    "* ~ 3k nodes in total (~2k nodes in a largest component)\n",
    "* ~ 160k links\n",
    "* ~ 16 communities with more than two nodes\n",
    "\n",
    "**Hashtags:**\n",
    "* ~55k unique hashtags used (latin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kl06cQ9qBQA"
   },
   "source": [
    "## Visualisation\n",
    "\n",
    "Similar to power law distribution on log-log scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgWl30YxqItw"
   },
   "source": [
    "<center>\n",
    "** Degree Distribution**  \n",
    "<img src=\"https://carlomarxdk.github.io/twitter/images/degree_distripution.png\"  width=\"60%\" />\n",
    "  </center>\n",
    "\n",
    "<center>\n",
    "** Followers Distribution**  \n",
    "<img src=\"https://carlomarxdk.github.io/twitter/plots/loglog-follow-dist.png\"  width=\"60%\" />\n",
    "  </center>\n",
    "  \n",
    "  <center>\n",
    "** Retweets Distribution**  \n",
    "<img src=\"https://carlomarxdk.github.io/twitter/plots/loglog-retw-dist.png\"  width=\"60%\" />\n",
    "  </center>\n",
    "  \n",
    "  <center>\n",
    "** Likes Distribution**  \n",
    "<img src=\"https://carlomarxdk.github.io/twitter/plots/loglog-like-dist.png\"  width=\"60%\" />\n",
    "  </center>\n",
    "\n",
    "## Activity\n",
    "\n",
    "![Activity per day](https://carlomarxdk.github.io/twitter/images/activity-image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wmT5k6aajND"
   },
   "source": [
    "# 3. Tools, theory and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ymof_WZQQVff"
   },
   "source": [
    "## The overall idea\n",
    "\n",
    "In overal, we have devided our project in 4 different topics:\n",
    "\n",
    "*  ** Network Analysis** (which included the analysis of centralities by looking at degrees, searching for communities & analyzing communities)\n",
    "*   **Hashtag analysis**, i.e. finding most important hashtags + looking at the usage throught out the period (here we used TF-IDF, tokenization and processing of unicode characters)\n",
    "* **Sentiment analysis** (to see how the sentiment changed throught out the period). Here we used tokenization and XXXXXXXXXX\n",
    "* **Vocabulary** (TF-IDF, tokenization, normalization, regular expressions and processing of unicode characters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwHQJxym3nr6"
   },
   "source": [
    "## Network Analysis\n",
    "(To see a detailed description and the code of what we did, see <a href = \"http://nbviewer.jupyter.org/github/carlomarxdk/twitter/blob/master/ipynb/Sentiment_and_network.ipynb#Building-Network\">here</a>)\n",
    "\n",
    "\n",
    "The network was creates with **accounts as nodes** and **retweets/mentions/replys** as links.\n",
    "\n",
    "This network was than analyzed throught different measures.\n",
    "\n",
    "#### <a href = \"http://nbviewer.jupyter.org/github/carlomarxdk/twitter/blob/master/ipynb/Sentiment_and_network.ipynb#Degree-Distribution\">Degree Distribution</a>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://carlomarxdk.github.io/twitter/plots/loglog-degree-distribution.png\"  width=\"60%\" />\n",
    "</center>\n",
    "\n",
    "This shows that the degree distribtion has the form of a power-law.\n",
    "\n",
    "#### <a href = \"https://carlomarxdk.github.io/twitter/network.html\">Network Plots (click!)</a>\n",
    "\n",
    "Plots can be found here:\n",
    "- Whole network (uses WebGL, might take some time to load, an might look quiet ugly tbh):\n",
    "    - languages: https://carlomarxdk.github.io/twitter/plots/languages_full_network.html\n",
    "    - communities: https://carlomarxdk.github.io/twitter/plots/communities_full_network.html\n",
    "- subnetwork (just account > 1000 followers):\n",
    "    - languages: https://carlomarxdk.github.io/twitter/plots/languages_foll_1000.html\n",
    "    - communities: https://carlomarxdk.github.io/twitter/plots/communities_foll_1000.html\n",
    "    \n",
    "The spring layout was chosen, since, even though we varied the parameters of the ForceAtlas2 quiet a bit, the spring_layout was the nicer layout in the end.\n",
    "\n",
    "The full (interactive-ish) plots of the network can be found on our website.\n",
    "\n",
    "Certain clusters are very apparent, and most of them seem to correspond to the language of the accounts. (this will be further discussed in the next sections)\n",
    "Especially the german and the spanish account seem to be very disconnected from the rest of the network and form their own subcommunities.\n",
    "\n",
    "Those subclusters are very connected, we could not extract any further structure from them.\n",
    "\n",
    "#### Modularity\n",
    "The <a href = \"http://nbviewer.jupyter.org/github/carlomarxdk/twitter/blob/master/ipynb/Sentiment_and_network.ipynb#Exploring-the-structure-of-the-network-even-further:\">**modularity**</a> of the network, according to languages and the best partition found using the <a href = \"https://www.google.com/search?q=python+louvain&oq=python+louvain&aqs=chrome..69i57j69i60l2j35i39l2j0.1783j0j7&sourceid=chrome&ie=UTF-8\">louvain algorithm</a>.\n",
    "\n",
    "####  <a href = \"http://nbviewer.jupyter.org/github/carlomarxdk/twitter/blob/master/ipynb/Sentiment_and_network.ipynb#Create-confusion-matrix-of-languages-to-communities-(calculated-by-the-louvain-algorithm)\">Confusion Matrix</a>\n",
    "\n",
    "To get a sense of how good the languages correspond to the communities calculated through the louvain-algorithm, the confusion matrix was calculated (communities with less than 2 members were disregarded).\n",
    "For the 3 big language-communities, the following results were found\n",
    "\n",
    "  <b>'de':</b> $98\\%$ were in the same community.\n",
    "  This further verifies that the community of german-troll accounts is very clustered and fairly disconnected from the rest of the troll-network.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <b>'en':</b> $55\\%$ are in the same/biggest community.\n",
    "  However, the rest is not spaced out evenly, but there is a seccond subcommunity forming with, and together they correspond to $66\\%$ of the nodes with language 'en'.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <b>'es':</b> $44\\%$ are member of the biggest community.\n",
    "  However, there is a second, equally big community, which also has mostly just 'es' members, together they correspond to $75\\%$ of the 'es'-nodes.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <b>'ru':</b> $43\\%$ are member of the biggest community.\n",
    "  However, this community is overlapping quiet a lot with a 'en'-community ($396$ nodes are 'ru' and $91$ are 'en'.)\n",
    "  3 of the 4 other big 'ru' communites are also overlapping quiet a bit with 'en' communities, however, there is one community with $89$ members, that has only $7$ 'en' members.\n",
    "  \n",
    "  \n",
    "  #### <a href = \"http://nbviewer.jupyter.org/github/carlomarxdk/twitter/blob/master/ipynb/Sentiment_and_network.ipynb#$C(k)$--plot\">$C(k)$ - plot</a>\n",
    "  Another way to visualize the underlying structure of the network is by plotting the local klustering coefficient $C$ in dependency of the degree $k$ of each node.\n",
    "This is than compared to the same plof of a random network with the same degree distribution.\n",
    "  \n",
    "  \n",
    "<center>\n",
    "<img src=\"https://carlomarxdk.github.io/twitter/plots/cl-k.png\"  width=\"60%\" />\n",
    "</center>\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ej4OsTVCG1R"
   },
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "To calculate the averag sentiment of the Tweets and profile descriptions, the <a href = \"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752\">LabMT 1.0</a> wordlist was used.\n",
    "This dataset includes the sentiment of a set words, the sentiment of a specific text can than be calculating by just taking the average of the sentiments of the words in the text.\n",
    "\n",
    "#### <a href = \"http://nbviewer.jupyter.org/github/carlomarxdk/twitter/blob/master/ipynb/Sentiment_and_network.ipynb#Analyzing-Tweet/Description-sentiments\"> Difference between tweet/description sentiments </a>\n",
    "At first it was analyzed, whether there is a difference between the sentiment distribtion of the tweets and the desciptions \n",
    "\n",
    "<center>\n",
    "<img src=\"https://carlomarxdk.github.io/twitter/plots/tw_sentiment.png\"  width=\"45%\" />\n",
    "<img src=\"https://carlomarxdk.github.io/twitter/plots/tw_sentiment.png\"  width=\"45%\" />\n",
    "</center>\n",
    "  \n",
    "  However, as can be seen in the 2 distribution plots, they seem to be quiet similar, so there is no significant difference.\n",
    "  \n",
    "  #### <a href =  \"http://nbviewer.jupyter.org/github/carlomarxdk/twitter/blob/master/ipynb/Sentiment_and_network.ipynb#Relationshipt-between-tweet-and-description-sentiments\" >Relationship between tweet and description sentiments</a>\n",
    "  It was also explored, whether there is a relationship between the Tweet and the Description sentiment of the according profile. \n",
    "  This was done by creating a heatmap.\n",
    "  <center>\n",
    "<img src=\"https://carlomarxdk.github.io/twitter/plots/tw_sents-desc_sents.png\"  width=\"70%\" />\n",
    "</center>\n",
    "\n",
    "However, was can be seen, there is no expicit relationship apparent. \n",
    "\n",
    "#### <a href = \"http://nbviewer.jupyter.org/github/carlomarxdk/twitter/blob/master/ipynb/Sentiment_and_network.ipynb#Average-tweet-sentiment-to-followers\"> Average tweet sentiment to followers </a>\n",
    "\n",
    "Another theory investigated was, whether there is a relationship between the average sentiment of the tweets of a specific account, and the followers of this account.\n",
    "The theory behind this idea was, that the network tried using very scandalous and therefore negative tweets to get a lot of attention, however, as can be seen in the heatmap, this wasnt the case.\n",
    "\n",
    "#### <a href = \"http://nbviewer.jupyter.org/github/carlomarxdk/twitter/blob/master/ipynb/Sentiment_and_network.ipynb##Retweets/#Likes-depending-on-sentiment\">#Retweets/#Likes depending on sentiment</a>\n",
    "\n",
    "It was also explored whether there is a relationship between the number of likes/retweets and the sentiment of a specific tweet. This was also done by using heatmaps\n",
    "\n",
    "<center>\n",
    "<img src=\"https://carlomarxdk.github.io/twitter/plots/sent-like-dist.png\"  width=\"45%\" />\n",
    "<img src=\"https://carlomarxdk.github.io/twitter/plots/sent-retw-dist.png\"  width=\"45%\" />\n",
    "</center>\n",
    "\n",
    "As can be seen in the heatmaps above, this also does not seem to be the case.\n",
    "\n",
    "### <a href = \"http://nbviewer.jupyter.org/github/carlomarxdk/twitter/blob/master/ipynb/Sentiment_and_network.ipynb#Sentiment-Analysis-according-to-Days\">Sentiment according to days</a>\n",
    "\n",
    "It was also explored, whether there is *anything* interesting about the average sentiment of each day. i.e.: are there specific drop offs in time, when the sentiment of the tweets is especially low/high? can those be related to specific events in real life?\n",
    "\n",
    "For this an interactive plotly-plot was created.\n",
    "Since the average sentiment/day of the troll network alone would not be very helpful, as a comparison baseline, the average sentiment of whole twitter (data retrieved from here https://hedonometer.org/about.html) was also plotted.\n",
    "\n",
    "Plot can be found here: https://carlomarxdk.github.io/twitter/plots/average_sentiment_per_day.html\n",
    "\n",
    "To make it easier to compare both series, they were normalized (average was set to 0 and divided by their standard error): https://carlomarxdk.github.io/twitter/plots/norm_sentiment_per_day.html \n",
    "\n",
    "In the beginning the sentiment of the troll network *'jumps around'* a  lot. \n",
    "This is due to the low number of tweets in this period.\n",
    "After that, the sentiment of the troll network is fairly well behaved, however, there still some significant differences to the average sentiment of whole twitter that stand out:\n",
    "\n",
    "In general, the sentiment of the network seems to be smaller, i.e. the tweets are more 'negative'.\n",
    "The sentiment for whole twitter seemt to change very slowly, and there are only very specific spikes, that mostly correspond either with holidays (thanksgiving, christmas) for *positive* spikes, or terror attacks for *negative* spikes.\n",
    "\n",
    "In comparison, in the sentiment time series of the troll network, there seem to be clear periods of lower sentiment (e.g.:10.May 2013 to 31.Jan 2014, or 19.Oct 2015 to the 27.Dec 2015). This may correspond to specific operations of the IRA, however it was not further investigated, so we were not able to definately verify that theory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhehHRmvDRYy"
   },
   "source": [
    "## <a href = \"http://nbviewer.jupyter.org/github/carlomarxdk/twitter/blob/master/ipynb/Sentiment_and_network.ipynb#Activity-per-day\">Acitvity per day</a>\n",
    "\n",
    "Another way, to detect certain influence operations of the troll network explored, was plotting the daily activity of the network. i.e.: The number of tweets per day of the whole network. This was also done by creating a plotly-plot.\n",
    "The resulting plot can be found here: https://carlomarxdk.github.io/twitter/plots/daily_activity.html\n",
    "\n",
    "To make it easier to explore this, the a log-plot of this was created:  https://carlomarxdk.github.io/twitter/plots/log-daily_activity.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fIbtOM_Qdci"
   },
   "source": [
    "## Hashtag analysis\n",
    "\n",
    "####Overview:\n",
    "\n",
    "*   Find the most used hashtags thought out the whole period\n",
    "*   Hashtag frequency per month \n",
    "\n",
    "In order to find the **most used/important hashtags**, we decided to use the TF-IDF values (i.e. statistics that shows how important the word is). \n",
    "\n",
    "1.   First of all, we had to extract all the hashtags that were used per day (see Part 1 in [Hashtag Analysis Notebook](https://nbviewer.jupyter.org/github/carlomarxdk/twitter/blob/master/ipynb/hashtag-analysis.ipynb))\n",
    "2.   Each day was considered to be a \"*document*\" (i.e. stored all the occurences of hashtag per day)\n",
    "3.   Another variable stored the vocabulary (i.e. set of unique hashtags)\n",
    "4.  First we counted the IDF. In the vocabulary, we added +1 for each occurence of the hashtag in the document (that is how we calculated the document frequence). After we applied the following formula \n",
    "$IDF = log(\\frac{NUMBER-OF-DAYS}{DF[word] + 1})$\n",
    "5. Then we calculated the Term Frequency. We had a variable that stored one big text consisting of all the hashtags from every day and we counted how many times word appears in this big document\n",
    "6. Then we calculated *TF-IDF* and used it to infer the importance of the word\n",
    "7. The first analysis was performed on the all hashtags. Later, we removed non-latin hashtags (i.e. for further analysis).\n",
    "\n",
    "Next we analysed **how hashtags were used throughout the period**. We focused only on  the hashtags (that were written in latin).\n",
    "1. As our dataset is quite huge and includes a lot of days, we decided to calculate the frequencies based on the months.\n",
    "2. First of all, we created a huge confusion matrix (rows were days and columns were unique hashtags). Each cell had a certain value (e.g. (16/02/2013, NEWS) cell stored the count of #news on 16th February 2013).\n",
    "3. Then we squezed/grouped rows by months. \n",
    "4. From this table we choose hashtags with the 100 largest TF-IDF value and displayed them in a plot (see Part 3 in [Hashtag Analysis Notebook](https://nbviewer.jupyter.org/github/carlomarxdk/twitter/blob/master/ipynb/hashtag-analysis.ipynb)) )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwpR9ZlfQdTi"
   },
   "source": [
    "## Sentiment analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhuRRkATQdMN"
   },
   "source": [
    "## Vocabulary analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73TPrTG8TMI7"
   },
   "source": [
    "Outline:\n",
    "\n",
    "1) what am I interested in here\n",
    "\n",
    "2) explain the tool\n",
    "\n",
    "3) apply the tool\n",
    "\n",
    "4) discuss the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ge93DYd2Qv6d"
   },
   "source": [
    "**Overview:**\n",
    "\n",
    "Tweets are mostly text messages composed of words. Therefore, it makes sense to look at the words that form the tweets and analyze them to find out what topics the tweets cover. This way we can see what the agency that runned the troll accounts wanted to share and spread inside the twitter community.\n",
    "\n",
    "**Used tools:**\n",
    "\n",
    "To get the meaningfull words from each tweet, we need to start by filtering out all twitter handles, URLs, and other special cases. We will start with **regular expressions** to get rid of all retweets, mentions of other twitter users, URLs, hashtags, and currencies. Often, the tweets contain even emojis (smileys) which needs to be removed as well. We will get rid of them with the help of their **unicode** representations (*\\U00010000* - *\\U0010ffff*).\n",
    "\n",
    "Next, we will **tokennize** the text of each tweet to get a set of unique words in that tweet. Each of the words will go through another filter to make the words lowercase and to exclude all english stopwords (from the [Stopwords Corpus](https://www.nltk.org/book/ch02.html)), numbers, and words shorter than 3 characters. Finally, we will **normalize** the tokens using the  WordNet **lemmatizer** to remove affixes (see chapter [3.6   Normalizing Text](https://www.nltk.org/book/ch02.html)). However, this process will reduce only the affixes for words that are in the WordNet dictionary. We also considered using **stemming** but as the stemming algorithms are not perfect and in most of our words would generate non-existing words or words that are hard to understand, it would make our analysis much harder, therefore, we will not use that tool in our analysis.\n",
    "\n",
    "With the normalized tokens for each tweet ready, we can start with **TF-IDF** that will help us find the most important words in the tweets. Here, we will first group the tweets into bins representing each calendar month. Then we will calculate the TF-IDF for each month separately, taking **each tweet as a single document**. We do this as one month is not too short nor too long to cover the most important events in a reasonable scale. Groupping by week or even smaller time periods would likely introduce some bias that we are not interested in. We focus on the long-term trends  to find the general intensions of the agency to (maybe) influence the way of thinking and manipulating public opinion by focusing on a certain topic.\n",
    "\n",
    "With the TF-IDF frequencies for each word, we can generate **wordclouds** to represent the most common words used in tweets for the specific month. We display the **100 most common words** for each month. We will start with narrowing our focus only on years where there was a significant tweeting activity (at least thousand tweets a month). This will give result in analyzing years 2014 to 2018. The resulting wordclouds are the ones presented on the main page in the *Vocabulary Analysis* section, part *Wordclouds*.\n",
    "\n",
    "To extend the analysis and create a better overview of the use of specific vocabulary, we will plot the frequency of the most common words for in the the whole tweet history. This way, the end user can see the trends for different words (be it mentions of Obama, Trump, or Clinton).\n",
    "\n",
    "**Tools application:**\n",
    "\n",
    "The implementation of the above mentioned tools is accessible in [this jupyter notebook](https://github.com/carlomarxdk/twitter/blob/master/ipynb/Vocabulary_analysis_notebook.ipynb). We start with Part 1 - Preprocessing that contain all from cleaning the tweets to tokenization and the definition of TF-IDF functions. Part 2 contains implementation for creating and saving wordclouds. Part 3 creates auxiliary csv files with preprocessed data to avoid problems with exceeding memory. The code in Part 4 calculates the general TF-IDF for all tweets  where tweets groupped by month represent one document. In Part 5 we create a plot with stacked bars for the additional long-term analysis.\n",
    "\n",
    "**The outcome:**\n",
    "\n",
    "We found out that the troll accounts followed the most publicly known cases and affairs, having a lot of tweets about the US 2016 presidential election. Moreover, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqiZGLFwajQW"
   },
   "source": [
    "# 4. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPcSV97LJ-xw"
   },
   "source": [
    "**Vocabulary, sentiments and hashtags**\n",
    "\n",
    "During our project work, we managed to do an extensive analysis of the vocabulary, sentiments and hashtags. More than that, we were able to documents the changes thgought the year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sdltVotBJv4P"
   },
   "source": [
    "**Communities**\n",
    "\n",
    "Based on teh analysis of the communities, we could find that communities are correlated with languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QugIyBSdJBmZ"
   },
   "source": [
    "**Work with Raw Data**\n",
    "\n",
    "The most issues with working on our dataset was the size of the raw data. Even though it is alread ypreprocessed and arranged, a lot of programms and python packages were not able to process it. Thus, a lot of time was used to learn how to chunk the information and how to devide the data into multiple subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zU47B09xJcIV"
   },
   "source": [
    "## Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WxVhlf7RJDIw"
   },
   "source": [
    "**Correlation with real events**\n",
    "\n",
    "One thing that we were plannign to do is to correlate our analysis to the real events. We did try to briefly discuss it. However, due to the limited time and resources we were not able to make an elaborate analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-gaTKe8LUeg"
   },
   "source": [
    "**Combining vocabulary with sentiments**\n",
    "\n",
    "One thing that would be nice to extend out project would be to combine our vocabulary results with sentiment analysis. This way, we could get some interesting insights into the agency interference. Were they using the trolls to benefit someone throughout the whole period, or did they support different opinions through time"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tweets_Explanatory.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
